{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The handwritten digit recognition is the ability of computers to recognize human handwritten digits. It is a hard tast for the machine becuase handwritten digigits are not perfect and can be made with many different flavors. The handwritten digit recognition is the solution to this problem which uses the image of a digit and recognize the digit present in the image.\n",
    "\n",
    "In this mini project, we are going to implement a handwritten digit recognition app using the MNIST dataset. We will be using a special type of deep neural network that is Convolutional Neural Networks. In the end, we are going to build a GUI in which you can draw the digit and recognize it straight away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data cannot be fed directly into the model so we need to perform som operations\n",
    "and process the data to make it ready for our neural network. The dimension of the training data is\n",
    "60000,28,28). The CNN model will require one more dimension so we shape the matrix to shape (60000,28,28,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our CNN model in Python data science project. A CNN model generally consists of convolutional and pooling layers. It works better for data that are represented as grid structures, this is the reason why CNN works well for image classification problems. The dropout layer is used to deactivate some of the neurons and while training, it reduces offer fitting of the model. We will then compile the model with the Adadelta optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3),activation='relu', input_shape = input_shape))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adadelta(),metrics=['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes some time to train the model. After training, we save the weights and model definition in the ‘mnist.h5’ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:10:30.921103: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - ETA: 0s - loss: 0.6465 - Accuracy: 0.1128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:10:52.497938: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 23s 42ms/step - loss: 0.6465 - Accuracy: 0.1128 - val_loss: 0.5948 - val_Accuracy: 0.1364\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 23s 49ms/step - loss: 0.5353 - Accuracy: 0.1262 - val_loss: 0.4534 - val_Accuracy: 0.1180\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 24s 51ms/step - loss: 0.4094 - Accuracy: 0.1338 - val_loss: 0.3533 - val_Accuracy: 0.1243\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 25s 53ms/step - loss: 0.3534 - Accuracy: 0.1473 - val_loss: 0.3275 - val_Accuracy: 0.2133\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 25s 52ms/step - loss: 0.3391 - Accuracy: 0.1663 - val_loss: 0.3201 - val_Accuracy: 0.2997\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 23s 50ms/step - loss: 0.3332 - Accuracy: 0.1877 - val_loss: 0.3148 - val_Accuracy: 0.3776\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 25s 52ms/step - loss: 0.3280 - Accuracy: 0.2117 - val_loss: 0.3095 - val_Accuracy: 0.4554\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 23s 50ms/step - loss: 0.3222 - Accuracy: 0.2395 - val_loss: 0.3040 - val_Accuracy: 0.5102\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 24s 52ms/step - loss: 0.3169 - Accuracy: 0.2653 - val_loss: 0.2983 - val_Accuracy: 0.5685\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.3111 - Accuracy: 0.2974 - val_loss: 0.2921 - val_Accuracy: 0.6311\n",
      "The model has sucessfully trained\n",
      "Saving the model as mnist.h5\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "print(\"The model has sucessfully trained\")\n",
    "model.save('mnist.h5')\n",
    "print(\"Saving the model as mnist.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10,000 images in our dataset which will be used to evaluate how good our model works. The testing data was not involved in the training of the data therefore, it is new data for our model. The MNIST dataset is well balanced so we can get around 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2921387553215027\n",
      "Test Accuracy: 0.631100058555603\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print(\"Test Accuracy:\",score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini project, we are able to test our handwritten digit recognition. However, the accuracy is not attracting which mean there is more room to improve it such as hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37e48532695005f6b8be481dca9311b0406960c07b920c9bcff5b3361c45626c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
